{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([82, 400])\n"
     ]
    }
   ],
   "source": [
    "plikembd = open(\"../embeddings_small.txt\")\n",
    "\n",
    "embeddingscuda_chars = {}\n",
    "embeddingscuda_chars['ðŸˆ³'] = 0\n",
    "\n",
    "embeddingscuda = torch.tensor([]).to(\"cuda:0\")\n",
    "embeddingscuda =  torch.cat((embeddingscuda, torch.zeros((400)).to(\"cuda:0\")), 0)\n",
    "\n",
    "#print(embeddingscuda.shape)\n",
    "\n",
    "for cnt, line in enumerate(plikembd.readlines()):\n",
    "    exp1 = line.replace(\"\\n\", \"\").split(\":\")\n",
    "    \n",
    "    znak = exp1[0]\n",
    "    embedding = torch.tensor([float(liczba) for liczba in exp1[1].replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\\t\")[:-1]]).to(\"cuda:0\")\n",
    "    \n",
    "    embeddingscuda_chars[znak] = cnt+1\n",
    "    embeddingscuda = torch.cat((embeddingscuda, embedding), 0)\n",
    "\n",
    "embeddingscuda = embeddingscuda.reshape(82, -1)\n",
    "\n",
    "print(embeddingscuda.shape)\n",
    "#print(embeddingscuda[embeddingscuda_chars['a']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.0851,  1.4174, -0.8968,  ..., -0.5574,  1.2953, -0.1756],\n",
       "        [ 0.9934, -0.4756, -0.5951,  ...,  0.8698,  0.2920,  0.0168],\n",
       "        ...,\n",
       "        [-0.2883,  0.0300, -0.3970,  ...,  0.0964, -0.2076, -0.5465],\n",
       "        [ 0.2481, -0.3300, -0.0937,  ...,  0.3352, -0.1689,  0.2105],\n",
       "        [ 0.2100, -0.0179, -0.3131,  ...,  0.0312, -0.2439,  0.2052]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddingscuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, dim_1, dim_2):\n",
    "        super(PositionalEmbeddings, self).__init__()\n",
    "        \n",
    "        self.positional_encs = nn.Parameter(torch.zeros(dim_1, dim_2))\n",
    "#         print(self.w.device)\n",
    "\n",
    "\n",
    "        for pos in range(dim_1):\n",
    "            for i in range(0, dim_2, 2):\n",
    "                self.positional_encs[pos, i] = math.sin(pos / (10000 ** ((i) / dim_2)))\n",
    "                self.positional_encs[pos, i + 1] = math.cos(pos / (10000 ** ((i + 1) / dim_2)))\n",
    "    \n",
    "        \n",
    "        self.dim_1 = dim_1\n",
    "        self.dim_2 = dim_2\n",
    "        \n",
    "        \n",
    "    def forward(self, embd):\n",
    "        return embd+self.positional_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEmbeddings(82, 400).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = pe(embeddingscuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = xx.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d54ae9d1de54ec59ede0821edf43ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22b83976b00>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(zz, aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/380/1*2vyKzFlzIHfSmOU_lnQE4A.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.MH = nn.MultiheadAttention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
