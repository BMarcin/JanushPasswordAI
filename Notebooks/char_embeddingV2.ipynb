{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--np-oHc2RVM"
   },
   "source": [
    "# Char Embedding dla JanushPasswordGAN\n",
    "*Notebook miał być krótszy 🙌*\n",
    "\n",
    "## Zadanie\n",
    "Na podstawie wszystkich haseł wyznaczyć wartości do nauki sieci neuronowej. Po nauczeniu sieci otrzymamy wektory zależności dla poszczególnych znaków.\n",
    "\n",
    "## Podzadania\n",
    "### Znalezienie unikalnych znaków w zbiorze alfabetu haseł\n",
    "Bierzemy wszystkie hasła i tworzymy zbiór unikalnych znaków, jakie były tam użyte.\n",
    "##### Info\n",
    "Funkcja dodatkowo musi porządkować znaki według kolejności alfabetycznej (liczby będą na początku).\n",
    "##### Przykład\n",
    "Nasz alfabet składa się ze słów: \"abc\" oraz \"abbd\". Na wyjściu otrzymamy wektor: \n",
    "```\n",
    "['a', 'b', 'c', 'd']\n",
    "```\n",
    "\n",
    "### Dla każdego znaku wyznaczyć One Hot Vector\n",
    "Dla każdego unikalnego znaku (dlatego wymagana jest funkcja powyżej) wyznaczamy one hot vector.\n",
    "#### Co to jest one hot vector?\n",
    "One hot vector jest to wektor 0 i 1 (zer i jedynek), który będzie jednoznacznie identyfikować każdy znak z podanego alfabetu. W każdym takim wektorze znajduje się jedna 1 (jedynka) i każdy jest długości wektora unikalnych znaków alfabetu. Jedynka występuje w tym miejscu, pod którym znajduje się dany znak w wektorze unikalnych znaków.\n",
    "##### Info\n",
    "W tak utworzonej przestrzenii można będzie wyróżnić wektor pusty - nie oznaczający żadnego ze znaków. Wektor taki będzie składał się z samych 0 (zer).\n",
    "##### Przykład\n",
    "Nasz alfabet składa się ze słów: \"abc\" oraz \"abbd\". Dla takiego alfabetu wyznaczamy unikalne znaki (funkcja powyżej) i otrzymujemy wektor ['a', 'b', 'c', 'd']. Zgodnie z definicją one hot vector'a wektor każdego znaku będzie długości wektora unikalnych znaków, czyli w tym przypadku 4. Wyznaczone one-hot-vectory dla tego przykładu powinny wyglądać następująco:\n",
    "```\n",
    "{\n",
    "    'a': [1, 0, 0, 0],\n",
    "    'b': [0, 1, 0, 0],\n",
    "    'c': [0, 0, 1, 0],\n",
    "    'd': [0, 0, 0, 1]\n",
    "}\n",
    "```\n",
    "\n",
    "### Dla każdego znaku wyznaczyć unikalne ID klasy - numeracja\n",
    "To zadanie jest dosyć proste. Sieć wyznaczająca Character Embedding będzie wykorzystywać funkje SoftMax oraz CrossEntropy (więcej info poniżej 🙃). Funkcja CrossEntropy jako jeden z argumentów przyjmuje \"klasy\" do których ma dopasować wyniki. W naszym przypadku jedna klasa to będzie jeden znak. Całe zadanie sprowadza się jedynie do utworzenia słownika, w którym pod indeksami znajdziemy znaki, a jako ich wartości kolejne cyfry.\n",
    "###### Przykład\n",
    "```\n",
    "{\n",
    "    'a': 0,\n",
    "    'b': 1,\n",
    "    'c': 2\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "### Dla każdego hasła wyznaczyć wektor one hot wektorów\n",
    "W tym zadaniu dosłownie musimy przepisać hasło składające się ze znaków na wektor one hot vector'ów. Chodzi o to, by stworzyć reprezentację każdego hasła w formie bardziej siecio neuronowej 😎.\n",
    "##### Przykład\n",
    "Nasz alfabet składa się ze słów: \"abc\" oraz \"abbd\". Wyznaczamy dla niego unikalne znaki wg. pierwszej funkcji z tego notebook'a i następnie one hot vectory dla każdego znaku (one hot vector'y będzie trzeba podać jako argument wejściowy dla tej funkcji). Na wyjściu powinniśmy otrzymać dwie macierze: jedną dla slowa \"abc\" i drugą dla \"abbd\".\n",
    "```\n",
    "[\n",
    "    [\n",
    "        [1, 0, 0, 0], #znak \"a\"\n",
    "        [0, 1, 0, 0], #znak \"b\"\n",
    "        [0, 0, 1, 0]  #znak \"c\"\n",
    "    ],\n",
    "    [\n",
    "        [1, 0, 0, 0], #znak \"a\"\n",
    "        [0, 1, 0, 0], #znak \"b\"\n",
    "        [0, 1, 0, 0], #znak \"b\"\n",
    "        [0, 0, 0, 1]  #znak \"d\"\n",
    "    ]\n",
    "]\n",
    "    \n",
    "```\n",
    "\n",
    "###  Wyznaczenie wielkości okna dla sieci CE (Character Embedding)\n",
    "Chodzi o znalezienie minimalnej wielkości okna, która pozwoli na prawidłowe wykonanie CE.\n",
    "\n",
    "#### Jak to ma działać?\n",
    "CE ma za zadanie wyznaczyć zależności między literami. Z tego też powodu należy każde słowo przepuścić przez SSN (Sztuczną Sieć Neuronową) na kilka przypadków (będzie ich tyle ile jest znaków w danym słowie). W sieci musi się pojawić coś na wejściu i coś na wyjściu. Jako, że SSN ma się nauczyć zależności między znakami, to będziemy operować na znakach. Dla pierwszego przypadku zabieramy ze słowa ostatni znak i umieszczamy jego one hot vector na wyjściu. Następnie wyznaczamy liczbę wejść do sieci, a będzie ich dwukrotna liczba znaków, które zostały po zabraniu tego ostatniego. Całe wejście dzielimy na dwie części - górne i dolne. Do górnego trafią one hot vector'y znaków poprzedzających, ten który jest na wyjściu, a resztę (część dolną) uzupełniamy wartościami pustymi (one hot vector z samymi 0 (zerami)). Dla kolejnego przypadku przesuwamy każdy znak \"w dół\". To oznacza, że na pierwszym neuronie wejściowym będzie teraz one hot vector zerowy, który był w poprzednim przypadku w dolnej części, a na drugim wejściu znak, który był wcześniej na pierwszym wejściu. Na wyjście sieci przeskakuje znak poprzedzający ten, który się znajdował wcześniej na wyjściu. Znak, który znajdował się na wyjściu przeskakuje do dolnej części na pierwsze wejście tej części.\n",
    "\n",
    "#### Więc gdzie w tym wszystkim wielkość okna?\n",
    "Wielkość okna musi pozwalać na swobodne wyznaczenie wszystkich przypadków, tak by znaki z których składa się dane słowo mogłby być poprzerzucane.\n",
    "\n",
    "##### Przykład\n",
    "Poniższy rysunek obrazuje wyznaczenie wielkości okna i rozpisanie przypadków dla słowa \"abc\". Jak widzimy dla słowa składającego się z 3 znaków wielkość okna musi wynosić 4.\n",
    "![Opis trzech przypadków](https://i.imgur.com/TQOwdIX.png)\n",
    "\n",
    "### CBOW\n",
    "Stworzenie sieci w stylu CBOW, by wyznaczyć zależności między znakami i stworzyć wektory dla każdego znaku.\n",
    "\n",
    "Schemat sieci\n",
    "![Schemat CBOW](https://i.imgur.com/jdTjKTS.png)\n",
    "\n",
    "#### Oznaczenia\n",
    "tx - tensor wejściowy zawierający one hot vector (array).\n",
    "\n",
    "h1 - neuron warstwy ukrytej.\n",
    "\n",
    "y1 - tensor wyjściowy zawierający one hot vector (array).\n",
    "\n",
    "\n",
    "#### Właściwie po co to całe ciboł?\n",
    "CBOW ma za zadanie wyznaczyć wektory dla znaków.\n",
    "\n",
    "#### Dobra, to gdzie w tej sieci te wektory?\n",
    "Wektory dla znaków wejściowych opisują wagi znajdujące się pomiędzy warstwą ukrytą, a wyjściem.\n",
    "##### Przykład\n",
    "Dla przykładu słowa \"abc\" będzie następujący opis wektorów dla znaków\n",
    "```\n",
    "[wh1y1, wh2y1, wh3y1], #znak \"a\"\n",
    "[wh1y2, wh2y2, wh3y2], #znak \"b\"\n",
    "[wh1y2, wh2y3, wh3y3]  #znak \"c\"\n",
    "```\n",
    "\n",
    "#### \"Mechanika\" CBOW\n",
    "W sieci możemy zaobserwować swego rodzaju kompresję danych i ich dekompresję (dla przykładu \"abc\" nie zaobserwujemy tego efektu). Kompresja następuje pomiędzy warstwą wejściową, a warstwą ukrytą (z wielu danych robimy mniejszą ich ilość). Dekompresja następuje między warstwą ukrytą, a warstwą wyjściową (z małej ilości danych robimy ich większą porcję). Daje nam to możliwość odtwarzania(rownoznaczne z dekompresją) słowa na podstawie wektorów. Liczba wektorów będzie wynosić tyle ile jest neuronów w warstwie ukrytej.\n",
    "\n",
    "#### Ale jak to wektory mają opisywać wagi, skoro wagi się ustalą na samym końcu uczenia i dla wszystkich znaków będą te same? aka. yyyy jakie to ma zależności?\n",
    "\n",
    "Sam miałem trochę problemów w zrozumieniu na jakiej podstawie można stwierdzić który wektor wag znajdujących się między warstwą ukrytą, a warstwą wyjściową odpowiada któremu znakowi. Odpowiedź jest banalnie prosta. Nie patrzmy na metodę CBOW, jako działającą od lewej do prawej tylko na odwrót. Dla danego znaku, który jest na wyjściu znaczenie w obliczeniach ma tylko jeden wektor, ze względu na kodowanie znaku jako one hot vector - ten, dla którego jako wynik przeliczeń (warstwy ukrytej i wag) wynosi 1. Dla reszty wyniki mają wynosić 0. Oto cała magia 🤗.\n",
    "\n",
    "##### Przykład \n",
    "![Przykład1](https://i.imgur.com/pnwQpDV.png)\n",
    "\n",
    "### Kompletny schemat sieci do wyznaczenia wektorów \n",
    "![Schemat sieci do wyznaczenia wektorów](https://i.imgur.com/r2sg5ZP.png)\n",
    "\n",
    "[Dokumentacja funkcji CrossEntropyLoss()](https://pytorch.org/docs/stable/nn.html#crossentropyloss)\n",
    "\n",
    "#### Opis wg. mnie\n",
    "Kombinacja funkcji Softmax i CrossEntropy ma za zadanie wartościom uzyskanych z sieci (y1,y2,y3,y4) przypisać wartość przynależności do danej klasy (podzadanie wyznaczenia klas). Im niższa wartość tym wartość bardziej przynależy do danej klasy.\n",
    "\n",
    "##### Przykład\n",
    "Jak widać wyżej w schemacie dla klasy 'a' (klasa znaku 'a') najniższą wartość, co stanowi o przynależności testowanej klasy. Testowanie klasy polega na obliczeniu wartości 'loss' za pomocą CrossEntropyLoss().\n",
    "\n",
    "#### Do czego właściwie w CE potrzebujemy Softmax'u i CrossEntropy?\n",
    "W sumie to jedynie do lepszego procesu uczenia i jak byśmy sie uparli to możemy sobie posprawdzać jakie jest prawdopodobieństwo wystąpienia jakiegoś znaku po danej sekwencji znaków"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4c3bfXZQy8jB"
   },
   "source": [
    "# Kod robiący magię part1 - przygotowanie wartości do sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzJ8syRPzBb2"
   },
   "source": [
    "## Importy\n",
    "Importujemy bibliotekę Numpy, która jest potrzebna dla części pierwszej naszego CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PchVJKMi2NwW"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilVJUQaEzGCz"
   },
   "source": [
    "## Funkcje\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEQYf-gzzJjZ"
   },
   "source": [
    "### Znalezienie unikalnych znaków\n",
    "Wejściem będzie wektor wyrazów\n",
    "Wyjściem jest wektor unikalnych znaków\n",
    "\n",
    "Lepiej rozpisany kod, co by łatwiej go zrozumieć:\n",
    "```\n",
    "def find_unique(wektor):\n",
    "  zwrot = []\n",
    "  for word in wektor:\n",
    "    for letter in word:\n",
    "      if letter not in zwrot:\n",
    "        zwrot.append(letter)\n",
    "  return zwrot\n",
    "  #PS. gdzieś tu jeszcze sorted() by musiało być, ale jako że edytuje to miesiąc później to nwm 😆\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1946,
     "status": "ok",
     "timestamp": 1554534013817,
     "user": {
      "displayName": "Marcin Borzymowski",
      "photoUrl": "https://lh3.googleusercontent.com/-cn5cz3qzzCE/AAAAAAAAAAI/AAAAAAAAeY4/AlgtNekgbTE/s64/photo.jpg",
      "userId": "08472151881814245002"
     },
     "user_tz": -120
    },
    "id": "I6FONpF9zSzX",
    "outputId": "9eed8dbe-da98-46a0-d8f6-215e447aa526"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'c', 'd', 'e', 'f'], dtype='<U1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_unique(wektor):\n",
    "    return np.array(sorted(list({letter for word in wektor for letter in word})))\n",
    "  \n",
    "find_unique(['abcf', 'abcd', 'abcde'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mSIiGizV1BDI"
   },
   "source": [
    "### Wyznaczenie One Hot Vectors\n",
    "Wejściem jest wektor unikalnych znaków\n",
    "\n",
    "Wyjściem jest słownik z one hot vector'ów podpisanych pod znak, który reprezentują\n",
    "\n",
    "Lepiej rozpisany kod, co by łatwiej go zrozumieć\n",
    "```\n",
    "def make_one_hot_vectors(wektor):\n",
    "  zwrot = {}\n",
    "  for cnt in range(len(wektor)):\n",
    "    vec = []\n",
    "    for ii in range(len(wektor)):\n",
    "      if ii == cnt:\n",
    "        vec.append(1)\n",
    "      else:\n",
    "        vec.append(0)\n",
    "    zwrot[wektor[cnt]] = vec\n",
    "  return zwrot\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1939,
     "status": "ok",
     "timestamp": 1554534013818,
     "user": {
      "displayName": "Marcin Borzymowski",
      "photoUrl": "https://lh3.googleusercontent.com/-cn5cz3qzzCE/AAAAAAAAAAI/AAAAAAAAeY4/AlgtNekgbTE/s64/photo.jpg",
      "userId": "08472151881814245002"
     },
     "user_tz": -120
    },
    "id": "0bTer6YC1FT8",
    "outputId": "94d7f803-2f19-41cf-e275-564c21f76565"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [1, 0, 0, 0, 0],\n",
       " 'b': [0, 1, 0, 0, 0],\n",
       " 'c': [0, 0, 1, 0, 0],\n",
       " 'd': [0, 0, 0, 1, 0],\n",
       " 'e': [0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_one_hot_vectors(wektor):\n",
    "    return {wektor[cnt]:[1 if ii == cnt else 0 for ii in range(len(wektor))] for cnt in range(len(wektor))}\n",
    "  \n",
    "make_one_hot_vectors(['a', 'b', 'c', 'd', 'e'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyznaczenie ID klasy\n",
    "Wejściem są wyznaczone wcześniej one hot vectory dla unikalnych znaków\n",
    "Wyjściem jest słownik, którego kluczami są znaki dla których wyznaczono one hot vectory, a kluczami ID klas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_class_id(one_hot_vectors):\n",
    "    zwrot = {}\n",
    "    \n",
    "    for cnt, one_hot_vector in enumerate(one_hot_vectors):\n",
    "        zwrot[one_hot_vector] = cnt\n",
    "        \n",
    "    return zwrot\n",
    "\n",
    "make_class_id(make_one_hot_vectors(['a', 'b', 'c', 'd', 'e']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gd4VzKq_pvc"
   },
   "source": [
    "### Wyznaczenie wektora one hot wektorów\n",
    "Wejście to słowo oraz słownik wcześniej wyznaczonych one hot wektorów dla znaków\n",
    "\n",
    "Wyjście to tablica one hot vectorów dla słowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1932,
     "status": "ok",
     "timestamp": 1554534013819,
     "user": {
      "displayName": "Marcin Borzymowski",
      "photoUrl": "https://lh3.googleusercontent.com/-cn5cz3qzzCE/AAAAAAAAAAI/AAAAAAAAeY4/AlgtNekgbTE/s64/photo.jpg",
      "userId": "08472151881814245002"
     },
     "user_tz": -120
    },
    "id": "k9ZnB1OP_uti",
    "outputId": "4bc14e25-a7cf-40c5-d970-e58691428880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]]\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]]\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "def make_word_vector(word, onehotvectors):\n",
    "    return np.array([onehotvectors[letter] for letter in word])\n",
    "\n",
    "for slowo in ['abc', 'abcd', 'abcde']:\n",
    "    print(make_word_vector(slowo, make_one_hot_vectors(['a', 'b', 'c', 'd', 'e'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaokxM4dA_MB"
   },
   "source": [
    "### Wyznaczenie wielkości okna oraz wszelkich kombinacji dla słowa w CE aka. wyznaczenie wartości do nauki sieci\n",
    "Generalnie było dużo rozkmin jak to zrobić, bo dużo się dzieje w tym. \n",
    "\n",
    "Musimy przepisać nasze wektory one hot wektorów na bezpośrednie wartości, które będą podane do sieci zachowujac wszelkie kombinacje znaków. \n",
    "\n",
    "Zobrazowałem działanie funkcji na pierwszym obrazie w tym notebook'u - tworzy ona kombinacje wartości.\n",
    "\n",
    "Wejściem jest one hot wektor słowa\n",
    "Wyjściem jest:\n",
    "+ inputs dla sieci neuronowej\n",
    "+ outputs dla sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1245,
     "status": "ok",
     "timestamp": 1554536630896,
     "user": {
      "displayName": "Marcin Borzymowski",
      "photoUrl": "https://lh3.googleusercontent.com/-cn5cz3qzzCE/AAAAAAAAAAI/AAAAAAAAeY4/AlgtNekgbTE/s64/photo.jpg",
      "userId": "08472151881814245002"
     },
     "user_tz": -120
    },
    "id": "r79u8xpVBHH3",
    "outputId": "667e2d57-d3eb-487e-cf76-c6dfc74c259c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEJSCIA\n",
      "[[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]\n",
      "  [0 1 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]\n",
      "  [0 0 1]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 1 0]\n",
      "  [0 0 1]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "\n",
      "WYJSCIA\n",
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def get_min_window_size(wordlen):\n",
    "    return 2*(wordlen-1)\n",
    "\n",
    "\n",
    "def make_pre_char_embedding(wektoryslow, windowsize, fill = 0):\n",
    "    \n",
    "    ''' definicje wejsc i wyjsc '''\n",
    "    inns = []\n",
    "    outs = []\n",
    "    \n",
    "    ''' lecimy po kazdym wektorze one hot vectorow '''\n",
    "    for wektor in wektoryslow:\n",
    "        ''' dla kazdego wektora sprawdzamy minimalna dlugosc okna, by sprawdzic czy jest zgodnosc z argumentem \"windowsize\" '''\n",
    "        min_window_size = get_min_window_size(len(wektor))\n",
    "\n",
    "        ''' sprawdzenie poprawności podanych danych '''\n",
    "        if min_window_size > windowsize:\n",
    "            raise Exception(\"Bledna wielkosc okna, powinna wynosic minimum: \" + str(min_window_size) + \", a mam: \" + str(windowsize))\n",
    "        \n",
    "        ''' nie obsługujemy słów krótszych od 3 '''\n",
    "        if len(wektor) < 3:\n",
    "            raise Exception(\"Wektor zbyt krotki. Dlugosc powinna wynosic minimum 3\")\n",
    "        \n",
    "        ''' lokalne wartości inputs i outputs '''\n",
    "        ''' w nich  przechowujemy \"kombinacje\" dla wszystkich znaków jednego słowa'''\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "\n",
    "        ''' tutaj jest bardzo ważna wartość wyliczana '''\n",
    "        ''' mając wielkość okna = 8, a słowo wielkości 3 musimy tak ułożyć znaki tego słowa, '''\n",
    "        ''' by znajdowały się one na środkowych pozycjach '''\n",
    "        ''' ta wartość jest indeksem, o który należy przesunąć znaki słowa '''\n",
    "        beginfill = int((windowsize-min_window_size)/2)\n",
    "\n",
    "        ''' outed zawiera index one hot vectora słowa, które znajduje się na wyjściu sieci '''\n",
    "        outed = len(wektor)- 1\n",
    "        \n",
    "        ''' zbudowanie pierwotnego okna '''\n",
    "        ''' początek okna wypełniamy one hot vectorami znaków,'''\n",
    "        ''' a resztę zerami '''\n",
    "        window = [wektor[x-beginfill] if (x < len(wektor)+beginfill-1) and (x > beginfill - 1) else [fill for xx in range(len(wektor[0]))] for x in range(windowsize)]\n",
    "\n",
    "        ''' dodajemy do tablic '''\n",
    "        inputs.append(window)\n",
    "        outputs.append(wektor[outed])\n",
    "\n",
    "        ''' i tutaj główna pętla przekształceń '''\n",
    "        ''' dużo rozkmin było jak zrobić wszystkie kombinacje dla danego słowa, '''\n",
    "        ''' ale python jest wspaniały jeżeli chodzi o operacje na tablicach, '''\n",
    "        ''' więc przesunięcie wszystkich znaków o jeden do przodu to po prostu pierwsza instrukcja pętli 💚 '''\n",
    "        ''' następnie robimy podmiankę wartości wyjściowej '''\n",
    "        ''' i zamieniamy indeks wyjścia '''\n",
    "        for x in range(len(wektor) - 1):\n",
    "            window = [window[-1]] + window[:-1]\n",
    "            window[outed+beginfill+x] = wektor[outed]\n",
    "            outed = outed - 1\n",
    "            #print(window)\n",
    "\n",
    "            inputs.append(window)\n",
    "            outputs.append(wektor[outed])\n",
    "        \n",
    "        \n",
    "        ''' inputy i outputy dla konkretnego słowa dodajemy do inputów i outputów całościowych '''\n",
    "        inns = inns + inputs\n",
    "        outs = outs + outputs\n",
    "    \n",
    "    return np.array(inns, dtype=int), np.array(outs, dtype=int)\n",
    "    #return inputs, outputs\n",
    "\n",
    "inputs, outputs = make_pre_char_embedding([make_word_vector(\"abc\", make_one_hot_vectors(find_unique(['abc'])))], 8)\n",
    "#inputs, outputs = make_pre_char_embedding([make_word_vector(\"abc\", make_one_hot_vectors(find_unique(['abc']))), make_word_vector(\"abcde\", make_one_hot_vectors(find_unique(['abcde'])))], 8)\n",
    "print(\"WEJSCIA\")\n",
    "print(inputs)\n",
    "print(\"\\nWYJSCIA\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bIBUU7GSjixY"
   },
   "source": [
    "### Funkcje pomocnicze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMtV7x_Cjkc7"
   },
   "outputs": [],
   "source": [
    "def find_longest_word(wektor):\n",
    "    longest = 0\n",
    "    for slowo in wektor:\n",
    "        if len(slowo) > longest:\n",
    "            longest = len(slowo)\n",
    "    return longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje tłumaczące różne postacie znaków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate_class_to_sign(classid, classdict):\n",
    "    for item in classdict:\n",
    "        if classdict[item] == classid:\n",
    "            return item\n",
    "        \n",
    "translate_class_to_sign(1, make_class_id(make_one_hot_vectors(['a', 'b', 'c', 'd', 'e'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate_sign_to_class(sign, classdict):\n",
    "    return classdict[sign]\n",
    "\n",
    "translate_sign_to_class('b', make_class_id(make_one_hot_vectors(['a', 'b', 'c', 'd', 'e'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate_one_hot_vector_to_sign(one_hot_vector, one_hot_vectors):\n",
    "    for item in one_hot_vectors:\n",
    "        if np.array_equal(np.array(one_hot_vectors[item]), one_hot_vector):\n",
    "            return item\n",
    "        \n",
    "translate_one_hot_vector_to_sign(np.array([0,1,0,0,0]), make_one_hot_vectors(['a', 'b', 'c', 'd', 'e']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19rc9MjJcreR"
   },
   "source": [
    "# Job/Magic\n",
    "## Info\n",
    "\n",
    "Jest tutaj małe demo prezentujące działanie funkcji, które zostały utworzone wyżej 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7058
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1050,
     "status": "ok",
     "timestamp": 1554537279606,
     "user": {
      "displayName": "Marcin Borzymowski",
      "photoUrl": "https://lh3.googleusercontent.com/-cn5cz3qzzCE/AAAAAAAAAAI/AAAAAAAAeY4/AlgtNekgbTE/s64/photo.jpg",
      "userId": "08472151881814245002"
     },
     "user_tz": -120
    },
    "id": "AdMImBQvcte5",
    "outputId": "d49e9004-a235-4edb-d929-21129d2c7e7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIKALNE ZNAKI\n",
      "['a' 'b' 'c' 'd' 'e']\n",
      "\n",
      "\n",
      "ONE-HOT-VECTORS\n",
      "a [1, 0, 0, 0, 0]\n",
      "b [0, 1, 0, 0, 0]\n",
      "c [0, 0, 1, 0, 0]\n",
      "d [0, 0, 0, 1, 0]\n",
      "e [0, 0, 0, 0, 1]\n",
      "\n",
      "\n",
      "HASLA ZAPISANE ONE-HOT-WEKTORAMI\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]]\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]]\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n",
      "\n",
      "\n",
      "NAJDLUZSZY WYRAZ:  5\n",
      "\n",
      "\n",
      "MIN WINDOW SIZE:  8\n",
      "\n",
      "\n",
      "INPUTY DLA SIECI\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n",
      "\n",
      "\n",
      "OUTPUTY DLA SIECI\n",
      "[0 0 1 0 0]\n",
      "[0 1 0 0 0]\n",
      "[1 0 0 0 0]\n",
      "[0 0 0 1 0]\n",
      "[0 0 1 0 0]\n",
      "[0 1 0 0 0]\n",
      "[1 0 0 0 0]\n",
      "[0 0 0 0 1]\n",
      "[0 0 0 1 0]\n",
      "[0 0 1 0 0]\n",
      "[0 1 0 0 0]\n",
      "[1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "''' lista naszych slow '''\n",
    "#hasla = [\"123456\", \n",
    "#         \"neo24\", \n",
    "#         \"qwerty\", \n",
    "#         \"monika\", \n",
    "#         \"1235\", \n",
    "#         \"misiek\"]\n",
    "\n",
    "hasla = [\"abc\", \"abcd\", \"abcde\"]\n",
    "\n",
    "\n",
    "\n",
    "''' wyszukujemy unikalne znaki i tworzymy one hot wektory '''\n",
    "unique = find_unique(hasla)\n",
    "onehots = make_one_hot_vectors(unique)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' przepisujemy slowa na one hot vectory ''' \n",
    "word_vectors = []\n",
    "\n",
    "for haslo in hasla:\n",
    "    word_vectors.append(make_word_vector(haslo, onehots))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "''' znajdujemy najdluzsze slowo i minimalna wielkosc okna ''' \n",
    "longestword = find_longest_word(hasla)\n",
    "min_window = get_min_window_size(longestword)\n",
    "\n",
    "''' tworzymy wartosci do uczenia dla sieci '''\n",
    "inputs, outputs = make_pre_char_embedding(word_vectors ,min_window)\n",
    "  \n",
    "    \n",
    "    \n",
    "''' printy na wszystko, co tu sie dzialo '''    \n",
    "print(\"UNIKALNE ZNAKI\")  \n",
    "print(unique)\n",
    "\n",
    "print(\"\\n\\nONE-HOT-VECTORS\")\n",
    "for onehot in onehots:\n",
    "    print(onehot, onehots[onehot])\n",
    "    \n",
    "print(\"\\n\\nHASLA ZAPISANE ONE-HOT-WEKTORAMI\")\n",
    "for word_vector in word_vectors:\n",
    "    print(word_vector)\n",
    "    \n",
    "print(\"\\n\\nNAJDLUZSZY WYRAZ: \", longestword) \n",
    "\n",
    "print(\"\\n\\nMIN WINDOW SIZE: \", min_window)\n",
    "\n",
    "print(\"\\n\\nINPUTY DLA SIECI\")\n",
    "for inn in inputs:\n",
    "    print(inn)\n",
    "\n",
    "print(\"\\n\\nOUTPUTY DLA SIECI\")\n",
    "for out in outputs:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GdpMEobAUq9I"
   },
   "source": [
    "# Kot 🐱 robiący magię part2 - SSN \n",
    "W tej części najfajniejsza zabawa. Zaczniemy od wprowadzenia całego modelu sieci i omówienia go na prostym przykładzie, czyli hasła \"abc\". \n",
    "\n",
    "Całość oparta na frameworku PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ufu2gNYSUq9m"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zbudowanie modelu\n",
    "\n",
    "Klasa w konstruktorze przyjmuje dwa parametry. Pierwszym jest ilość wejść do sieci, która musi odpowiadać minimalnenej wielkości okna. Drugim argumentem jest ilość wyjść, która musi być równa najdłuższemu wyrazowi, co jest równe wielkości jednego one hot vector'a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.fc_in = nn.Linear(in_features=inputs, out_features=4)\n",
    "        self.fc_out = nn.Linear(in_features=4, out_features=outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych trenujących"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funkcja pomocnicza do tłumaczenia one hot wektora na klasę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_labels_to_classes(labels, classdict, one_hot_vectors):\n",
    "    return np.array([translate_sign_to_class(translate_one_hot_vector_to_sign(item, one_hot_vectors), classdict) for item in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy tutaj swego rodzaju zdublowanie podsumowania części pierwszej dotyczącej character embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, 'a': 6, 'b': 7, 'c': 8, 'd': 9, 'e': 10, 'g': 11, 'h': 12, 'i': 13, 'j': 14, 'k': 15, 'l': 16, 'm': 17, 'n': 18, 'o': 19, 'p': 20, 'q': 21, 'r': 22, 's': 23, 't': 24, 'u': 25, 'w': 26, 'y': 27, 'z': 28} \n",
      "\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]] \n",
      "\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] \n",
      "\n",
      "[ 5  4  3  2  1  0  3  1 19 10 18 27 24 22 10 26 21  6 15 13 18 19 17  4\n",
      "  2  1  0 15 10 13 23 13 17 27 20 25 15  6 28  6 18 13 16 19 22  6 15  6\n",
      " 13 23  6 15  3  2  1  0 18 13  8 22  6 17 22 19 13 23  6 24 15 10 24 22\n",
      "  6  7 11 18 25 23 17  6 23 15 10 17 19 24  6 24  6 10  7 22 10 20  8  6\n",
      " 15  0  6 13 23  6 15  0  0  0  0  0  0 28 22 19 11 10 28 22 11  6 15 28\n",
      " 23 27 17 28 23 25 10 24  6 17  6 15 16  6 22 20 16  6 12  8 13 17  6 15\n",
      " 13 18 19 22 10 26 10 26 21  2  1  0 15 10 24 14 19 26 23 10  9 10  8 22\n",
      " 10 17]\n"
     ]
    }
   ],
   "source": [
    "''' zebranie slow '''\n",
    "slowa = [\"123456\", \n",
    "         \"neo24\", \n",
    "         \"qwerty\", \n",
    "         \"monika\", \n",
    "         \"1235\", \n",
    "         \"misiek\", \n",
    "         \"zakupy\", \n",
    "         \"karolina\", \n",
    "         \"kasia\", \n",
    "         \"1234\", \n",
    "         \"marcin\", \n",
    "         \"tasior\", \n",
    "         \"bartek\", \n",
    "         \"samsung\", \n",
    "         \"tomek\", \n",
    "         \"beata\", \n",
    "         \"kacper\", \n",
    "         \"kasia1\", \n",
    "         \"111111\", \n",
    "         \"grzegorz\", \n",
    "         \"myszka\", \n",
    "         \"mateusz\", \n",
    "         \"pralka\", \n",
    "         \"michal\", \n",
    "         \"weronika\", \n",
    "         \"123qwe\", \n",
    "         \"wojtek\", \n",
    "         \"mercedes\"]\n",
    "\n",
    "\n",
    "\n",
    "''' znalezienie unikalnych znakow i utworzenie one hot vectorow dla znakow '''\n",
    "unikalne = find_unique(slowa)\n",
    "onehotvectory = make_one_hot_vectors(unikalne)\n",
    "\n",
    "\n",
    "''' przepisanie slow na one hot vectory '''\n",
    "wyrazy = [make_word_vector(slowo, onehotvectory) for slowo in slowa]\n",
    "\n",
    "\n",
    "\n",
    "''' zbudowanie klas dla poszczegolnych znakow'''\n",
    "classdict = make_class_id(onehotvectory)\n",
    "print(classdict, '\\n')\n",
    "\n",
    "\n",
    "''' znalezienie najdluzszego slowa i minimalnej wielkosci okna '''\n",
    "najdluzszeslowo = find_longest_word(slowa)\n",
    "minimalnawielkoscokna = get_min_window_size(najdluzszeslowo)\n",
    "\n",
    "''' przygotowanie danych do uczenia '''\n",
    "training_data, training_data_labels = make_pre_char_embedding(wyrazy, minimalnawielkoscokna)\n",
    "    \n",
    "print(training_data, '\\n')\n",
    "print(training_data_labels, '\\n')\n",
    "\n",
    "training_data_classes = translate_labels_to_classes(training_data_labels, classdict, onehotvectory)\n",
    "print(training_data_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prezentacja postaci danych\n",
    "Czyli pokazanie wielkości macierzy, jakie trafiają do sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ksztalt macierzy przed przeksztalceniem (170, 14, 29) \n",
      "\n",
      "macierz przed przeksztalceniem [[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]] \n",
      "\n",
      "ksztalt macierzy po przeksztalceniu (170, 406) \n",
      "\n",
      "macierz do podania do sieci [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print('ksztalt macierzy przed przeksztalceniem', training_data.shape, '\\n')\n",
    "print('macierz przed przeksztalceniem', training_data, '\\n')\n",
    "training_data = training_data.reshape(training_data.shape[0], -1)\n",
    "print('ksztalt macierzy po przeksztalceniu', training_data.shape, '\\n')\n",
    "print('macierz do podania do sieci', training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ilosc probek do uczenia sieci 406\n",
      "ilosc probek na wyjsciu sieci 170\n"
     ]
    }
   ],
   "source": [
    "print('ilosc probek do uczenia sieci', training_data.shape[1])\n",
    "print('ilosc probek na wyjsciu sieci', training_data_classes.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jeżeli dostępne jest GPU to przenosimy dane na GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utworzenie modelu sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW = nn.DataParallel(CBOW(training_data.shape[1], len(unikalne)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie tensorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Tensor z wartościami wejściowymi '''\n",
    "TensorX = torch.Tensor(training_data).to(device)\n",
    "\n",
    "''' Tensor z klasami, które mają się pokazywać na wyjściu '''\n",
    "TensorClass = torch.tensor(training_data_classes).long().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postać tensorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([ 5,  4,  3,  2,  1,  0,  3,  1, 19, 10, 18, 27, 24, 22, 10, 26, 21,  6,\n",
      "        15, 13, 18, 19, 17,  4,  2,  1,  0, 15, 10, 13, 23, 13, 17, 27, 20, 25,\n",
      "        15,  6, 28,  6, 18, 13, 16, 19, 22,  6, 15,  6, 13, 23,  6, 15,  3,  2,\n",
      "         1,  0, 18, 13,  8, 22,  6, 17, 22, 19, 13, 23,  6, 24, 15, 10, 24, 22,\n",
      "         6,  7, 11, 18, 25, 23, 17,  6, 23, 15, 10, 17, 19, 24,  6, 24,  6, 10,\n",
      "         7, 22, 10, 20,  8,  6, 15,  0,  6, 13, 23,  6, 15,  0,  0,  0,  0,  0,\n",
      "         0, 28, 22, 19, 11, 10, 28, 22, 11,  6, 15, 28, 23, 27, 17, 28, 23, 25,\n",
      "        10, 24,  6, 17,  6, 15, 16,  6, 22, 20, 16,  6, 12,  8, 13, 17,  6, 15,\n",
      "        13, 18, 19, 22, 10, 26, 10, 26, 21,  2,  1,  0, 15, 10, 24, 14, 19, 26,\n",
      "        23, 10,  9, 10,  8, 22, 10, 17], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(TensorX)\n",
    "print(TensorClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wybranie optymizera oraz funkcji LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Adam optimizer '''\n",
    "optimizer = optim.Adam(CBOW.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "''' CrossEntropy jako loss '''\n",
    "criterion = nn.CrossEntropyLoss() #z softmaxem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opisanie obiektu sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CBOW(\n",
       "    (fc_in): Linear(in_features=406, out_features=4, bias=True)\n",
       "    (fc_out): Linear(in_features=4, out_features=29, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzenie tensora wejściowego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TensorX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzenie tensora wyjściowego (tensora klas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  4,  3,  2,  1,  0,  3,  1, 19, 10, 18, 27, 24, 22, 10, 26, 21,  6,\n",
       "        15, 13, 18, 19, 17,  4,  2,  1,  0, 15, 10, 13, 23, 13, 17, 27, 20, 25,\n",
       "        15,  6, 28,  6, 18, 13, 16, 19, 22,  6, 15,  6, 13, 23,  6, 15,  3,  2,\n",
       "         1,  0, 18, 13,  8, 22,  6, 17, 22, 19, 13, 23,  6, 24, 15, 10, 24, 22,\n",
       "         6,  7, 11, 18, 25, 23, 17,  6, 23, 15, 10, 17, 19, 24,  6, 24,  6, 10,\n",
       "         7, 22, 10, 20,  8,  6, 15,  0,  6, 13, 23,  6, 15,  0,  0,  0,  0,  0,\n",
       "         0, 28, 22, 19, 11, 10, 28, 22, 11,  6, 15, 28, 23, 27, 17, 28, 23, 25,\n",
       "        10, 24,  6, 17,  6, 15, 16,  6, 22, 20, 16,  6, 12,  8, 13, 17,  6, 15,\n",
       "        13, 18, 19, 22, 10, 26, 10, 26, 21,  2,  1,  0, 15, 10, 24, 14, 19, 26,\n",
       "        23, 10,  9, 10,  8, 22, 10, 17], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TensorClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wypisanie wielkości macierzy tensorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([170, 406])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TensorX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([170])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TensorClass.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wypisanie wielkości wag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 406])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBOW.module.fc_in.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBOW.module.fc_out.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pierwszego przeliczenia sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = CBOW(TensorX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie sieci 👽\n",
    "### Wyjaśnienie poszczególnych zależności\n",
    "```loss.backward(), optimizer.step(), optimizer.zero_grad(), CBOW.train()```\n",
    "\n",
    "pytorch umożliwia ustawienie sieci w tryb uczenia (bardziej złożony obliczeniowo) dlatego stosuje się na początku polecenie ```CBOW.train()```\n",
    "\n",
    "```optimizer.step()``` aktualizuje parametry bazując na obecnej wartości gradientu. Wywołując ```loss.backward()``` sumuje wartości gradientu dla każdego parametru, dlatego po każdym ```optimizer.step()``` powinniśmy go wyzerować wykorzystując ```optimizer.zero_grad()```.\n",
    "\n",
    "W niektórych typach sieci np. rekurencyjnych korzysta się z sumowania wartości gradientu i nie zeruje się jej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n"
     ]
    }
   ],
   "source": [
    "''' ustawiamy tablice do stworzenia wykresiku '''\n",
    "lossx = []\n",
    "lossy = []\n",
    "\n",
    "''' Właściwa nauka sieci '''\n",
    "for epoch in range(22000):\n",
    "    ''' Przestawienie obiektu sieci w tryb uczenia '''\n",
    "    CBOW.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ''' Przeliczenie wartości '''\n",
    "    y_ = CBOW(TensorX)\n",
    "    \n",
    "    ''' Obliczenie funkcji loss '''\n",
    "    loss = criterion(y_, TensorClass)\n",
    "    \n",
    "    ''' Dodanie loss do wykresiku '''\n",
    "    lossx.append(epoch)\n",
    "    lossy.append(loss)\n",
    "\n",
    "    ''' Wsteczna propagacja i aktualizacja parametrów przez optimizer'''\n",
    "    loss.backward(loss)\n",
    "    optimizer.step()\n",
    "    \n",
    "    ''' print '''\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyświetlenie wykresu funkcji LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29d07797400>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fix, ax = plt.subplots()\n",
    "ax.plot(lossx, lossy, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Końcowa wartość LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wypisanie wag - wektorów dla poszczególnych znaków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 2.1931,  0.8272, -2.0412, -0.6663],\n",
       "        [ 1.5130,  1.1437, -1.8022,  2.1213],\n",
       "        [ 1.9682, -1.6273, -1.8342, -1.4252],\n",
       "        [ 0.9122, -1.7326, -2.3486,  1.0160],\n",
       "        [ 0.7862, -1.4051, -0.8626,  2.5286],\n",
       "        [ 1.9869,  0.1184, -0.6159, -2.3581],\n",
       "        [-2.0315, -0.8995, -2.0305,  1.8326],\n",
       "        [ 1.5581, -2.1662,  2.4547, -0.7539],\n",
       "        [ 0.7055,  1.8532,  2.1081,  1.1137],\n",
       "        [-0.0718,  0.5995, -1.0464, -2.6285],\n",
       "        [-0.2473,  2.0792, -2.1034, -0.5287],\n",
       "        [-1.2192, -2.4789, -1.9915, -0.0878],\n",
       "        [ 1.9364,  0.8407,  2.1112, -0.3539],\n",
       "        [ 2.3690,  2.3451,  0.0171,  0.6382],\n",
       "        [ 1.3847, -2.0265,  1.7633,  1.4512],\n",
       "        [-2.1155, -1.5337, -0.6503, -2.2414],\n",
       "        [-0.8052,  2.2430,  0.6030,  0.3018],\n",
       "        [-0.9617, -2.1021,  2.0134, -1.5975],\n",
       "        [-1.9627,  1.5479, -0.3133, -1.7771],\n",
       "        [-1.5930,  1.2491,  2.6188, -1.7039],\n",
       "        [ 2.0714, -0.3166,  2.0823,  2.1153],\n",
       "        [ 2.5524, -1.9315,  0.1335,  1.5111],\n",
       "        [-1.3924, -2.3555,  2.1061,  2.2927],\n",
       "        [-1.8144,  1.0933,  1.1134,  2.3971],\n",
       "        [-2.4894, -1.0007,  0.9834,  0.5235],\n",
       "        [ 2.3345, -2.3615,  0.1773, -1.0012],\n",
       "        [ 1.1545, -0.2916,  1.8084, -2.4950],\n",
       "        [-0.0853, -1.2262, -2.1678, -1.9453],\n",
       "        [ 0.6804,  2.2914,  1.3325, -2.3767]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBOW.module.fc_out.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testowanie sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(40.2838, device='cuda:0')\n",
      "2 tensor(22.5190, device='cuda:0')\n",
      "3 tensor(52.6319, device='cuda:0')\n",
      "4 tensor(34.3751, device='cuda:0')\n",
      "5 tensor(19.5292, device='cuda:0')\n",
      "6 tensor(51.5873, device='cuda:0')\n",
      "a tensor(10.0097, device='cuda:0')\n",
      "b tensor(42.5548, device='cuda:0')\n",
      "c tensor(15.9288, device='cuda:0')\n",
      "d tensor(42.1251, device='cuda:0')\n",
      "e tensor(26.0722, device='cuda:0')\n",
      "g tensor(32.0403, device='cuda:0')\n",
      "h tensor(32.2573, device='cuda:0')\n",
      "i tensor(26.1492, device='cuda:0')\n",
      "j tensor(26.4934, device='cuda:0')\n",
      "k tensor(34.4740, device='cuda:0')\n",
      "l tensor(12.7101, device='cuda:0')\n",
      "m tensor(33.1420, device='cuda:0')\n",
      "n tensor(25.1086, device='cuda:0')\n",
      "o tensor(21.8157, device='cuda:0')\n",
      "p tensor(22.7463, device='cuda:0')\n",
      "q tensor(36.9912, device='cuda:0')\n",
      "r tensor(9.9928, device='cuda:0')\n",
      "s tensor(0.0001, device='cuda:0')\n",
      "t tensor(10.3798, device='cuda:0')\n",
      "u tensor(48.5107, device='cuda:0')\n",
      "w tensor(43.5843, device='cuda:0')\n",
      "y tensor(42.6082, device='cuda:0')\n",
      "z tensor(36.2496, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "''' przepisanie slowa na one hot vector '''\n",
    "wyrazy_testowe = [make_word_vector('mercedes', onehotvectory)]\n",
    "\n",
    "''' przepisanie slowa na wartosci przy zachowaniu wielkosci okna z procesu uczenia '''\n",
    "''' UWAGA - w tym procesie ze slowa podanego wyzej zabierana jest ostatnia litera '''\n",
    "''' zgodnie z definicja funkcji  \"make_pre_char_embedding\" ostatnia litera trafia na wyjscie sieci'''\n",
    "''' zatem wpisujac w zbiorze testowym slowo ze zbioru uczacego, na wyjsciu powinnismy otrzymac ostatni znak '''\n",
    "''' z podanego wyzej slowa 🙃 '''\n",
    "testing_data, testing_data_labels = make_pre_char_embedding(wyrazy_testowe, minimalnawielkoscokna)\n",
    "\n",
    "''' reshape wartosci '''\n",
    "''' wybieramy indeks slowa \"0\", bo chcemy miec pierwszy przypadek ulozenia wartosci wejsciowych dla sieci '''\n",
    "''' tzn. wszystkie litery na wejscie, oprocz ostatniej, ktora trafia na wyjscie '''\n",
    "testing_data = testing_data[0].reshape(1, -1)\n",
    "\n",
    "''' utworzenie tensora i wyslanie go do wlasciwego urzadzenia (CPU/GPU) '''\n",
    "TestingTensor = torch.Tensor(testing_data)\n",
    "TestingTensor = TestingTensor.to(device)\n",
    "\n",
    "''' dla CrossEntropy wyznaczana jest wartosc przynaleznosci do danej klasy '''\n",
    "''' im nizsza wartosc, tym lepsze dopasowanie '''\n",
    "''' do CrossEntropy mozemy podac tylko jedna wartosc do przeliczenia '''\n",
    "''' dlatego nalezy zrobic tablice tensorow wartosci wszystkich klas '''\n",
    "''' dla ktorych pozniej przeliczymy LOSS CrossEntropy '''\n",
    "TestingTensorsClass = [torch.tensor([x]).long().to(device) for x in range(len(unikalne))]\n",
    "\n",
    "''' bardzo wazne - w sieci nalezy wylaczyc tryb uczenia '''\n",
    "''' z tego co czytalem, wylacza on m.in. korzystanie z wartosci gradientow, co przyspiesza obliczenia '''\n",
    "''' wykorzystywac wartosc tylko wtedy, gdy nie planujemy uczyc sieci '''\n",
    "CBOW.eval()\n",
    "\n",
    "\n",
    "''' tryb wylaczonych gradientow '''\n",
    "with torch.no_grad():\n",
    "    ''' iterujemy przez wszystkie wartosci klas '''\n",
    "    for x in range(len(unikalne)):\n",
    "        ''' przeliczamy wartosc '''\n",
    "        y_ = CBOW(TestingTensor)\n",
    "        \n",
    "        ''' chcemy zrobic fajnego print'a, wiec tlumaczymy podawana klase na znak '''\n",
    "        ''' w drugim argumencie printa otrzymujemy nasz LOSS, '''\n",
    "        ''' ktory mowi jaki najprawdopodobniej jest ostatni znak podanego slowa '''\n",
    "        ''' (dla slowa ze zbioru uczacego powinien to byc ostatni znak podanego slowa) '''\n",
    "        print(translate_class_to_sign(x, classdict), criterion(y_, TestingTensorsClass[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z kodu wyżej zrobienie funkcji \n",
    "Tutaj generalnie jest taki myk, że słowo które chcemy przetestować jest bezpośrednio konwertowane na wektor one hot wektorów. Nie występuje tutaj dopasowywanie słowa do wielkości okna.\n",
    "\n",
    "#### Ale dlaczego?\n",
    "Rzecz w tym, że biorąc pod uwagę słowo \"abc\" przy wielkości okna = 8 nie bylibyśmy w stanie ułożyć wektorów tak, by pokazało przewidywany znak dla znaku pomiędzy znakami słowa.\n",
    "\n",
    "##### Przykład\n",
    "Chcemy wyznaczyć prawdopodobieństwo wystąpienia jakiegoś znaku pomiędzy znakami \"abb\" oraz \"cc\". Stosując dopasowywanie do wielkości okna moglibyśmy dopasowywać jedynie prawdopodobieństwo ostatniego znaku, albo innego typu ułożenia, ale na to trza pisać osobne funkcje 😜\n",
    "\n",
    "#### Jak to więcj działa\n",
    "Metoda ta wykorzystuje opisywany wyżej myk. Nie jest on do końca poprawny, ale działa, a jak działa i jest głupie, to nie jest aż takie głupie. Znaki naszego słowa są bezpośrednio konwertowane na one hot vectory. Oznacza to, że musimy wybrać jakiś znak, który nie wystąpi w zbiorze znaków. Odwołując się do zbioru naszych haseł jestem przekonany, że na pewno nie wystąpi w nim... emotka 🈳 \"japoński przycisk 'wolne miejsce'\".\n",
    "\n",
    "##### Przykład\n",
    "Mamy wielkość okna = 8, chcemy sprawdzić predykcję dla znaku pomiędzy \"abb\" oraz \"cc\". Nasze słowo musimy zatem zapisać jako:\n",
    "🈳abbcc🈳🈳.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(minimalnawielkoscokna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n 0.5373992919921875\n",
      "e 1.1352081298828125\n",
      "d 2.796807289123535\n",
      "z 3.4891624450683594\n",
      "l 6.445146560668945\n",
      "o 7.607002258300781\n",
      "y 7.849733352661133\n",
      "k 8.552597999572754\n",
      "1 8.79341983795166\n",
      "6 10.951892852783203\n",
      "i 11.683647155761719\n",
      "w 13.154324531555176\n",
      "a 15.852775573730469\n",
      "t 16.237091064453125\n",
      "3 18.450305938720703\n",
      "c 18.631845474243164\n",
      "h 19.0881404876709\n",
      "s 19.186553955078125\n",
      "2 19.213600158691406\n",
      "g 19.8337345123291\n",
      "m 19.9807071685791\n",
      "4 24.092668533325195\n",
      "u 25.20496368408203\n",
      "5 28.46202850341797\n",
      "b 31.598663330078125\n",
      "p 33.29864501953125\n",
      "j 33.79948425292969\n",
      "r 34.65837478637695\n",
      "q 35.61117172241211\n"
     ]
    }
   ],
   "source": [
    "''' FUNKCJA WYKORZYSTUJACA EMOTKI 😆 '''\n",
    "''' WYKORZYSTYWANA EMOTKA: 🈳, WE SE SKOPIUJ  '''\n",
    "def test_word(word, fill = 0):\n",
    "    word_one_hot = np.array([onehotvectory[letter] if letter != '🈳' else [fill for x in range(len(unikalne))] for letter in word])\n",
    "\n",
    "    if len(word_one_hot) != minimalnawielkoscokna:\n",
    "        raise Exception(\"Slowo o blednej dluosci wejsciowej\")\n",
    "\n",
    "    TestTensor = torch.Tensor(word_one_hot.reshape(1, -1)).to(device)\n",
    "\n",
    "    TestingTensorsClass = [torch.tensor([x]).long().to(device) for x in range(len(unikalne))]\n",
    "\n",
    "    prawdopodobienstwa = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x in range(len(unikalne)):\n",
    "            y_ = CBOW(TestTensor)\n",
    "            \n",
    "            ''' obliczenie CrossEntropyLoss i zapisanie do słownika '''\n",
    "            prawdopodobienstwa[translate_class_to_sign(x, classdict)] = criterion(y_, TestingTensorsClass[x]).item()\n",
    "            \n",
    "        ''' posortowanie i wypisanie predykcji '''\n",
    "        for item in sorted(prawdopodobienstwa, key=prawdopodobienstwa.get):\n",
    "            print(item, prawdopodobienstwa[item])\n",
    "            \n",
    "test_word(\"🈳mercedes🈳🈳🈳🈳🈳\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, 'a': 6, 'b': 7, 'c': 8, 'd': 9, 'e': 10, 'g': 11, 'h': 12, 'i': 13, 'j': 14, 'k': 15, 'l': 16, 'm': 17, 'n': 18, 'o': 19, 'p': 20, 'q': 21, 'r': 22, 's': 23, 't': 24, 'u': 25, 'w': 26, 'y': 27, 'z': 28}\n",
      "a oraz 1:  -0.20318055\n",
      "a oraz 2:  0.29019618\n",
      "a oraz 3:  -0.11692118\n",
      "a oraz 4:  0.55803335\n",
      "a oraz 5:  0.5505853\n",
      "a oraz 6:  -0.6505636\n",
      "a oraz a:  1.0000001\n",
      "a oraz b:  -0.58106524\n",
      "a oraz c:  -0.48864615\n",
      "a oraz d:  -0.3027108\n",
      "a oraz e:  0.18205574\n",
      "a oraz g:  0.71550846\n",
      "a oraz h:  -0.90853775\n",
      "a oraz i:  -0.48387635\n",
      "a oraz j:  -0.1617659\n",
      "a oraz k:  0.23408662\n",
      "a oraz l:  -0.12063607\n",
      "a oraz m:  -0.26033157\n",
      "a oraz n:  -0.00235564\n",
      "a oraz o:  -0.48232433\n",
      "a oraz p:  -0.3338954\n",
      "a oraz q:  -0.076077506\n",
      "a oraz r:  0.3336214\n",
      "a oraz s:  0.40507293\n",
      "a oraz t:  0.4806048\n",
      "a oraz u:  -0.39331108\n",
      "a oraz w:  -0.8871496\n",
      "a oraz y:  0.189663\n",
      "a oraz z:  -0.8224315\n"
     ]
    }
   ],
   "source": [
    "def similarity(letter1, letter2):\n",
    "    id1=0\n",
    "    id2=0\n",
    "    \n",
    "    for classid in classdict:\n",
    "        if classid == letter1:\n",
    "            id1 = classdict[classid]\n",
    "        \n",
    "        if classid == letter2:\n",
    "            id2 = classdict[classid]\n",
    "    \n",
    "    v1 = CBOW.cpu().module.fc_out.weight[id1].detach().numpy()\n",
    "    v2 = CBOW.cpu().module.fc_out.weight[id2].detach().numpy()\n",
    "    \n",
    "    d = np.dot(v1, v2)\n",
    "    \n",
    "    l1 = np.sqrt(np.sum(v1**2))\n",
    "    l2 = np.sqrt(np.sum(v2**2))\n",
    "    \n",
    "    return d/(l1*l2)\n",
    "\n",
    "\n",
    "literka = \"a\"\n",
    "\n",
    "print(classdict)\n",
    "\n",
    "for classid in classdict:\n",
    "    print(literka+\" oraz \"+classid+\": \", similarity(literka, classid))\n",
    "\n",
    "#print(similarity(\"z\", \"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ciekawostka 1\n",
    "Może emotki, to sposób na nowe i o wiele bezpieczniejsze hasła?\n",
    "\n",
    "![WP hasło emotkami](https://i.imgur.com/1pQVulX.png)\n",
    "\n",
    "W WP mamy filtry na pole z hasłem.\n",
    "\n",
    "A w 🍊🍊.net nie 😎\n",
    "\n",
    "![Morele.net](https://i.imgur.com/Nj5Sujm.png)\n",
    "\n",
    "Na konto da się normalnie zalogować i korzystać 😆\n",
    "\n",
    "#### Ciekawostka 2\n",
    "+ 🧑🏿 składa się z 4 znaków\n",
    "+ 💚 składa się z 2 znaków\n",
    "+ 👨‍👨‍👧‍👧 składa się z 11 znaków 😮\n",
    "\n",
    "#### Ciekawostka 3\n",
    "[Generator hasła emoji](https://passmoji.com/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "char_embeddingV2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
